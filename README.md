# ğŸ§  RAG PDF Q&A App using LangChain + Groq

ğŸš€ **Live Demo:** [Try it on Streamlit](https://rag-app-langchain-groq-jmv7qhq4cpfyzaj9zahwlb.streamlit.app/)  
ğŸ“¦ **GitHub Repo:** https://github.com/Hrithikdeep/rag-qa-langchain-groq

---

## ğŸ“¸ Demo Preview

| Upload PDF | Ask Questions | Smart Answers |
|------------|----------------|----------------|
| ![upload](<img width="250" height="150" alt="image" src="https://github.com/user-attachments/assets/da3c39fd-5047-4a13-a812-568e2da07f3e" />
) | ![question](<img width="250" height="150" alt="Screenshot 2025-08-05 at 11 55 12â€¯AM" src="https://github.com/user-attachments/assets/6e56414e-a79f-4e4b-a7c9-de72af7c259c" />
) | ![answer](<img width="250" height="150" alt="Screenshot 2025-08-05 at 11 55 12â€¯AM" src="https://github.com/user-attachments/assets/26e18d86-107a-4a9c-b0ee-1eb656fff99d" />
) |

---

## ğŸ“¦ What I Built

This is a full-stack **RAG (Retrieval-Augmented Generation) app** that:

âœ… Uploads any PDF  
âœ… Splits into text chunks  
âœ… Converts to embeddings using Hugging Face  
âœ… Saves vectors in FAISS  
âœ… Retrieves relevant chunks based on user question  
âœ… Uses **Groq (Mixtral LLM)** for context-aware answers  
âœ… Fully deployed on **Streamlit Cloud**

---

## ğŸ§  Why I Built This

Large language models (LLMs) forget documents quickly.  
This app uses **RAG** to "remind" the LLM by retrieving exact text chunks from the source document before generating answers â€” making responses more accurate and grounded.

---

## ğŸ”¨ Built With

- ğŸ§  [LangChain](https://github.com/langchain-ai/langchain)
- âš¡ [Groq LLM](https://console.groq.com)
- ğŸ¤— [Hugging Face Embeddings](https://huggingface.co/sentence-transformers)
- ğŸ” [FAISS Vector Store](https://github.com/facebookresearch/faiss)
- ğŸ–¥ï¸ [Streamlit](https://streamlit.io)

---

## ğŸ“ Project Structure

```
rag-qa-langchain-groq/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py              # Streamlit UI
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ rag_chain.py         # RAG logic (retrieve + generate)
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ secrets.toml         # Groq API key for Streamlit Cloud
â”œâ”€â”€ requirements.txt         # Clean dependencies
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ§ª Run Locally in 3 Minutes

```bash
# 1ï¸âƒ£ Clone the Repo
git clone https://github.com/Hrithikdeep/rag-qa-langchain-groq.git
cd rag-qa-langchain-groq

# 2ï¸âƒ£ Set Up Virtual Environment
python3 -m venv venv
source venv/bin/activate

# 3ï¸âƒ£ Install Required Packages
pip install -r requirements.txt

# 4ï¸âƒ£ Add Your Groq API Key
# Option A: for local dev
echo "GROQ_API_KEY=your_groq_api_key_here" > .env

# Option B: for Streamlit Cloud
mkdir -p .streamlit && echo 'GROQ_API_KEY = "your_groq_api_key_here"' > .streamlit/secrets.toml

# 5ï¸âƒ£ Run the App
streamlit run app/main.py
```

Then open http://localhost:8501 in your browser.

---

## ğŸ¤– How the App Works (Step-by-Step)

1. ğŸ“„ **Upload a PDF**
2. âœ‚ï¸ **Split it into small text chunks**
3. ğŸ¤— **Convert text into vector embeddings** using Hugging Face
4. ğŸ§  **Save to FAISS** vector database
5. â“ **User asks a question**
6. ğŸ” **Retrieve relevant chunks**
7. âš¡ **Groq Mixtral LLM generates final answer**

---

## ğŸ’¾ requirements.txt (Minimal)

```txt
streamlit
langchain==0.3.27
langchain-core
langchain-community
langchain-groq
sentence-transformers
faiss-cpu
pypdf
python-dotenv
```

---

## ğŸš€ Deploying to Streamlit Cloud

1. Push this project to GitHub
2. Go to [https://streamlit.io/cloud](https://streamlit.io/cloud)
3. Connect your GitHub repo
4. Add secret key in **Secrets** tab:

```toml
GROQ_API_KEY = "your_groq_api_key_here"
```

5. Click **Deploy**

âœ… Done â€” now your app runs live in the cloud.

---
## ğŸ™ Credits

- [@Hrithikdeep](https://github.com/Hrithikdeep)  
- Built with â¤ï¸ using LangChain + Groq + Streamlit
